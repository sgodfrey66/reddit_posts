{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from the Reddit API\n",
    "\n",
    "This module runs a script to import reddit posts, stores them in a DataFrame and outputs that DataFrame to a csv file for usage later.  \n",
    "\n",
    "To do this, it employs and a class object and some supporting functions which have been designed to interact with the Reddit HTTP JSON Application Programming Interface (API).  The class object is called ReadReddit and it is built to pull Reddity posts or listings.  It is built using the Requests Python [library](http://docs.python-requests.org/en/master/) for HTTP communication.\n",
    "\n",
    "ReadRedditPosts has the following attributes\n",
    "\n",
    "* url_base - the base URL for data pulls in this case 'http://www.reddit.com/'\n",
    "* url_ - the actual URL used to retrieve data from subreddit\n",
    "* no_posts_ - the number of posts returned after calling collect_posts\n",
    "* status_code_ - the HTTP status code returned after calling collect_posts\n",
    "* json_ = the json format of the web call content\n",
    "* after_ = the after parameter returned from a Reddit API\n",
    "\n",
    "and the following methods\n",
    "    \n",
    "* collect_posts(sub_grp = None, params = {}, headers = {}) - collecting posts data\n",
    "* return_posts() - return the individual posts as a list \n",
    "* return_post_keys() - return the keys of posts records\n",
    "* posts(features = []) - return a list of dictionaries containing posts data\n",
    "    \n",
    "Key functions are hit_reddit and write_data.  Hit_reddit takes inputs of a lists of subreddits and features and repeatedly calls the a variable instantiated as ReadReddit object to retrieve data.  The results are returned as a data frame and saved as a csv file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- https://docs.python.org/3/library/time.html\n",
    "- Practice SQL with pandas, Pt. 1 by Sam Stack (DC), Mark Popovich (SF)\n",
    "- https://stackoverflow.com/questions/775049/how-do-i-convert-seconds-to-hours-minutes-and-seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install psycopg2\n",
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephengodfrey/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from time import gmtime, strftime, sleep, localtime\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from pandas.io import sql\n",
    "import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to streamline min, max, type and null\n",
    "def print_summary(df):\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            col_type = df[column].dtype\n",
    "        except:\n",
    "            col_type = 'Unknown'\n",
    "        try:\n",
    "            col_min = df[column].min()\n",
    "        except:\n",
    "            col_type = 'Unknown'\n",
    "        try:\n",
    "            col_max = df[column].max()\n",
    "        except:\n",
    "            col_type = 'Unknown'\n",
    "   \n",
    "        print(\"Column: %15s  min: %15s  max: %15s  type: %15s  null: %15s\" % (column[:15], \n",
    "                str(col_min)[:15], str(col_max)[:15], str(col_type)[:15], str(df[column].isnull().sum()))[:15])\n",
    "\n",
    "\n",
    "# Function to hit the reddit API for specified subgroups and features to return\n",
    "def hit_reddit(sub_groups = [], features = [], calls = 15, inc_comm = False):\n",
    "    \n",
    "    # parameters for the API call\n",
    "    headers = {'user-agent': 'SteveG'}\n",
    "    params = {}\n",
    "    aft_lst = {}\n",
    "    # Calculate the sleep interval\n",
    "    slp_int = 1\n",
    "    \n",
    "    pst_lst = []\n",
    "    # for each of the calls\n",
    "    for i in range(calls):\n",
    "        # for each subreddit\n",
    "        print(\"working on call: \", i)\n",
    "        for j, sub in enumerate(sub_groups):\n",
    "            # If already called pass the after parameter to get latest posts\n",
    "            if i != 0:\n",
    "                params = {'after': aft_lst[j]}\n",
    "            # Call the ReadReddit object to get the posts in a list of dictionaries\n",
    "            posts = ReadRedditPosts()\n",
    "            posts.collect_posts(sub_grp=sub, params = params, headers = headers)\n",
    "            sub_post = posts.posts(features = features)\n",
    "            \n",
    "            #Include comments if flag set and permalink in features\n",
    "            if inc_comm and 'permalink' in features:\n",
    "                for sub_item in sub_post:\n",
    "                    if len(sub_item['permalink']) > 0:\n",
    "                        comm_string = ''\n",
    "                        comm = RedditComments()\n",
    "                        comm.collect_comments(url = sub_item['permalink'], headers = headers)\n",
    "                        # Add comments as one long string separated by three ;\n",
    "                        for comment in comm.comments(features=['body']):\n",
    "                            comm_string += comment['body'] + ';;;'\n",
    "                        sub_item['num_comments_cap'] = comm.no_comments_\n",
    "                        sub_item['comments'] = comm_string\n",
    "                        # pause before hitting the API again\n",
    "                        time.sleep(slp_int)   \n",
    "\n",
    "            pst_lst.extend(sub_post)\n",
    "            # Set the after value for the next call to the API\n",
    "            aft_lst[j] = posts.after_\n",
    "            # pause before hitting the API again\n",
    "            time.sleep(slp_int)   \n",
    "    \n",
    "    # Convert the list to a DataFrame and drop dups\n",
    "    df = pd.DataFrame(pst_lst)\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_data(df, data_path):\n",
    "    # assign a unique file name based on the current time\n",
    "    t_stmp = strftime(\"%d%b%Y_%H_%M\", localtime())\n",
    "    o_file = \"posts_\" + t_stmp + \".csv\"\n",
    "    df.to_csv(os.path.join(data_path, o_file), index = False)\n",
    "\n",
    "    \n",
    "def write_to_database(df, engine = None, table = None):\n",
    "    # write posts to the posts table\n",
    "    if engine == None:\n",
    "        return\n",
    "    else:\n",
    "        df.to_sql(table, con=engine, index=False, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadRedditPosts:\n",
    "    # Attributes of the data retrieval\n",
    "    url_base = 'http://www.reddit.com/'\n",
    "    url_ = None\n",
    "    no_posts_ = None\n",
    "    status_code_ = None\n",
    "    json_ = None\n",
    "    after_ = None\n",
    "    \n",
    "    # Initialization method\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # method to collect data from posts\n",
    "    def collect_posts(self, sub_grp = None, params = {}, headers = {}):\n",
    "        # Set the URL and save it to the class variable\n",
    "        url = self.url_base + 'r/' + sub_grp + '.json'\n",
    "        self.url_ = url\n",
    "        # Hit the API to get posts from this URL\n",
    "        res = requests.get(url, params = params, headers = headers)\n",
    "        # If 200 return\n",
    "        res_code_ = res.status_code\n",
    "        if res.status_code == 200:\n",
    "            self.json_ = res.json()\n",
    "            self.no_posts_ = len(self.json_['data']['children'])\n",
    "            self.after_ = self.json_['data']['after']\n",
    "            return res.json()\n",
    "        else:\n",
    "            return 'Data retrieval error: status code:' + str(res.status_code)\n",
    "\n",
    "    # Method to return the individual posts as a list    \n",
    "    def return_posts(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data['data']['children']\n",
    "    \n",
    "    # Method to return the dictionary keys for posts\n",
    "    def return_post_keys(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data['data']['children'][0]['data'].keys()\n",
    "\n",
    "    # Method to return a list of dictionaries of posts with specified fields\n",
    "    def posts(self, features = []):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        posts = []\n",
    "        # For every entry in the children posts add a dictionary to the list\n",
    "        for entry in data['data']['children']:\n",
    "            post = {}\n",
    "            # For each item in features create a dictionary key: value pair\n",
    "            for item in features:\n",
    "                try:\n",
    "                    post[item] = entry['data'][item]\n",
    "                except:\n",
    "                    post[item] = ''                   \n",
    "            posts.append(post)\n",
    "        return posts\n",
    "\n",
    "    \n",
    "class RedditComments:\n",
    "    # Attributes of the data retrieval\n",
    "    url_ = None\n",
    "    no_comments_ = None\n",
    "    status_code_ = None\n",
    "    json_ = None\n",
    "\n",
    "    \n",
    "    # Initialization method\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # method to collect data from posts\n",
    "    def collect_comments(self, url = None, params = {}, headers = {}):\n",
    "        # Use the provided URL and save it to the class variable\n",
    "        if url == None:\n",
    "            return\n",
    "        if url[:21] == 'http://www.reddit.com/':\n",
    "            url = url + '.json'\n",
    "        else:\n",
    "            url = 'http://www.reddit.com' + url +'.json'         \n",
    "        self.url_ = url\n",
    "        # Hit the API to get posts from this URL\n",
    "        res = requests.get(url, params = params, headers = headers)\n",
    "        # If 200 return\n",
    "        res_code_ = res.status_code\n",
    "        if res.status_code == 200:\n",
    "            self.json_ = res.json()\n",
    "            self.no_comments_ = len(self.json_[1]['data']['children'])\n",
    "            return res.json()\n",
    "        else:\n",
    "            return 'Data retrieval error: status code:' + str(res.status_code)\n",
    "\n",
    "    # Method to return the individual posts as a list    \n",
    "    def return_comments(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data[1]['data']['children']\n",
    "    \n",
    "    # Method to return the dictionary keys for posts\n",
    "    def return_comment_keys(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data[1]['data']['children'][0]['data'].keys()\n",
    "\n",
    "    # Method to return a list of dictionaries of posts with specified fields\n",
    "    def comments(self, features = []):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        comments = []\n",
    "        # For every entry in the children posts add a dictionary to the list\n",
    "        for entry in data[1]['data']['children']:\n",
    "            comment = {}\n",
    "            # For each item in features create a dictionary key: value pair\n",
    "            for item in features:\n",
    "                try:\n",
    "                    comment[item] = entry['data'][item]\n",
    "                except:\n",
    "                    comment[item] = ''                   \n",
    "            comments.append(comment)\n",
    "        return comments\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Establish parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters for retrieving reddit posts data\n",
    "sub_groups = ['relationships', 'diy','politics', 'woodworking']\n",
    "inc_list = ['name','subreddit','selftext','created_utc','author_fullname',\n",
    "           'title', 'num_comments','id', 'permalink']\n",
    "# Set relative data path\n",
    "data_path = \"../data\"\n",
    "\n",
    "# Database engine\n",
    "engine = create_engine('postgres://postgres:pass@54.69.8.168:5432')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from the reddit API and write to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on call:  0\n",
      "working on call:  1\n",
      "working on call:  2\n",
      "working on call:  3\n",
      "working on call:  4\n",
      "working on call:  5\n",
      "working on call:  6\n",
      "working on call:  7\n",
      "working on call:  8\n",
      "working on call:  9\n",
      "working on call:  10\n",
      "working on call:  11\n",
      "working on call:  12\n",
      "working on call:  13\n",
      "working on call:  14\n",
      "working on call:  15\n",
      "working on call:  16\n",
      "working on call:  17\n",
      "working on call:  18\n",
      "working on call:  19\n",
      "working on call:  20\n",
      "working on call:  21\n",
      "working on call:  22\n",
      "working on call:  23\n",
      "working on call:  24\n",
      "working on call:  25\n",
      "working on call:  26\n",
      "working on call:  27\n",
      "working on call:  28\n"
     ]
    }
   ],
   "source": [
    "# Return a dataframe of reddit posts and calculate time to run the function\n",
    "start_time = time.time()\n",
    "df =  hit_reddit(sub_groups = sub_groups, features = inc_list, calls = 50, inc_comm = True)\n",
    "# write_data(df, data_path)\n",
    "write_to_database(df, engine = engine, table = 'posts')\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the running time\n",
    "run_time = str(datetime.timedelta(seconds = end_time - start_time))\n",
    "print(\"run time: \", run_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tposts = ReadReddit()\n",
    "# tdata = tposts.collect_posts(sub_grp='politics',headers = {'user-agent': 'SteveG'})\n",
    "# print(tposts.return_post_keys())\n",
    "# tjson = tposts.json_\n",
    "# tjson['data']\n",
    "\n",
    "# print(tposts.posts(features=inc_list)[3]['permalink'])\n",
    "# comm = RedditComments()\n",
    "# comms = comm.collect_comments(url = tposts.posts(features=inc_list)[10]['permalink'], headers = {'user-agent': 'SteveG'})\n",
    "# comm.comments(features=['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the resulting DataFrame\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Value counts of subreddit\n",
    "df['subreddit'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of duplicated rows\n",
    "print(\"Duplicated rows: %d \\n\" % sum([int(i) for i in df.duplicated()]))\n",
    "\n",
    "#Print a summary of DataFrame columns\n",
    "print_summary(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicates in the selftext column\n",
    "print(\"There might be duplicates in %d rows\" % (len(df['selftext']) - len(set(df['selftext']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
