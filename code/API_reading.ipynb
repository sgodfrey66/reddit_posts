{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Reading-data-from-the-Reddit-API\" data-toc-modified-id=\"Reading-data-from-the-Reddit-API-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reading data from the Reddit API</a></span><ul class=\"toc-item\"><li><span><a href=\"#Class-objects\" data-toc-modified-id=\"Class-objects-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Class objects</a></span></li><li><span><a href=\"#Data-retrieval-and-storage\" data-toc-modified-id=\"Data-retrieval-and-storage-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data retrieval and storage</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>References</a></span></li></ul></li><li><span><a href=\"#Build-code-base\" data-toc-modified-id=\"Build-code-base-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Build code base</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Establish-functions\" data-toc-modified-id=\"Establish-functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Establish functions</a></span></li><li><span><a href=\"#Establish-classes\" data-toc-modified-id=\"Establish-classes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Establish classes</a></span></li><li><span><a href=\"#Establish-parameters\" data-toc-modified-id=\"Establish-parameters-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Establish parameters</a></span></li></ul></li><li><span><a href=\"#Read-data\" data-toc-modified-id=\"Read-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Read data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Retrieve-data-from-the-reddit-API-and-write-to-a-database\" data-toc-modified-id=\"Retrieve-data-from-the-reddit-API-and-write-to-a-database-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Retrieve data from the reddit API and write to a database</a></span></li><li><span><a href=\"#Examine-the-resulting-DataFrame\" data-toc-modified-id=\"Examine-the-resulting-DataFrame-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Examine the resulting DataFrame</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from the Reddit API\n",
    "\n",
    "This module runs a script to import reddit posts, stores them in a DataFrame and outputs that DataFrame to a PostgreSQL database which is hosted on cloud machine at Amazon Web Services (AWS).  \n",
    "\n",
    "\n",
    "## Class objects\n",
    "\n",
    "To retrieve posts, this code employs two class objects and some supporting functions which have been designed to interact with the Reddit HTTP JSON Application Programming Interface (API).  The class objects are ReadRedditPosts and RedditComments and they are developed to pull and store in memory subreddit posts or listings from the Reddit website.  They are built using the Requests Python [library](http://docs.python-requests.org/en/master/) for HTTP communication and provide several attributes and methods for extracting useful information.\n",
    "\n",
    "   * ReadRedditPosts attributes:\n",
    "       * url_base - the base URL for data pulls in this case 'http://www.reddit.com/'\n",
    "       * url_ - the actual URL used to retrieve data from subreddit\n",
    "       * no_posts_ - the number of posts returned after calling collect_posts\n",
    "       * status_code_ - the HTTP status code returned after calling collect_posts\n",
    "       * json_ = the json format of the web call content\n",
    "       * after_ = the after parameter returned from a Reddit API\n",
    "\n",
    "   * ReadRedditPosts methods:\n",
    "       * collect_posts() - collecting posts data\n",
    "       * return_posts() - return the individual posts as a list \n",
    "       * return_post_keys() - return the keys of posts records\n",
    "       * posts() - return a list of dictionaries containing posts data\n",
    "\n",
    "\n",
    "   * ReadComments attributes:\n",
    "       * url_ - the URL used to retrieve comment data for a particular post\n",
    "       * no_comments_ - the number of comments returned after calling collect_comments\n",
    "       * status_code_ - the HTTP status code returned after calling collect_comments\n",
    "       * json_ = the json format of the web call content\n",
    "\n",
    "   * ReadComments methods:\n",
    "       * collect_comments() - collecting comments data for a particular post\n",
    "       * return_comments() - return the individual comments as a list \n",
    "       * return_comment_keys() - return the keys of comments records\n",
    "       * comments() - return a list of dictionaries containing comments data\n",
    "\n",
    "\n",
    "\n",
    "## Data retrieval and storage\n",
    "\n",
    "Key functions are hit_reddit() and write_data().  Hit_reddit() takes inputs of a lists of subreddits and features and repeatedly calls the a variable instantiated as ReadReddit object to retrieve data.  If the inc_comm paramater is set to True, the function will also search for comments related to an individual post using the permalink data element which contains a URL with comment data.\n",
    "\n",
    "The results are returned as a data frame and saved to a PostgreSQL data using an insert SQL database statement.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "The data were collected by reading from the Reddit API on Saturday March 30, 2019 from 8:30 am - 10:30 am and 6:15 pm – 8:15 pm and again Wednesday April 3 from 9:00 - 9:30 AM. The following fields were extracted from the post and comments message and stored in a SQL database.  \n",
    "\n",
    "|Column        |Description    |\n",
    "|-----------------|--------------------|\n",
    "|author_fullname  | ID of the author   |\n",
    "|comments| comments to this post retrieved using the permalink element|\n",
    "|created_utc| creation time|\n",
    "|id| ID associated with this post|\n",
    "|name| expanded ID tag|\n",
    "|num_comments| numeric field in the post message|\n",
    "|num_comments_cap| number of comments captured when reading comments|\n",
    "|permalink| URL housing comments|\n",
    "|selftext| Text of the post|\n",
    "|subreddit| subreddit category housing this post|\n",
    "|title| post title|\n",
    "\n",
    "## References\n",
    "\n",
    "- https://docs.python.org/3/library/time.html\n",
    "- Practice SQL with pandas, Pt. 1 by Sam Stack (DC), Mark Popovich (SF)\n",
    "- https://stackoverflow.com/questions/775049/how-do-i-convert-seconds-to-hours-minutes-and-seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build code base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install psycopg2\n",
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephengodfrey/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from time import gmtime, strftime, sleep, localtime\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from pandas.io import sql\n",
    "import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to streamline min, max, type and null\n",
    "def print_summary(df):\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            col_type = df[column].dtype\n",
    "        except:\n",
    "            col_type = 'Unknown'\n",
    "        try:\n",
    "            col_min = df[column].min()\n",
    "        except:\n",
    "            col_type = 'Unknown'\n",
    "        try:\n",
    "            col_max = df[column].max()\n",
    "        except:\n",
    "            col_type = 'Unknown'\n",
    "   \n",
    "        print(\"Column: %15s  min: %15s  max: %15s  type: %15s  null: %15s\" % (column[:15], \n",
    "                str(col_min)[:15], str(col_max)[:15], str(col_type)[:15], str(df[column].isnull().sum()))[:15])\n",
    "\n",
    "\n",
    "# Function to hit the reddit API for specified subgroups and features to return\n",
    "def hit_reddit(sub_groups = [], features = [], calls = 15, inc_comm = False):\n",
    "    \n",
    "    # parameters for the API call\n",
    "    headers = {'user-agent': 'SteveG'}\n",
    "    params = {}\n",
    "    aft_lst = {}\n",
    "    # Calculate the sleep interval\n",
    "    slp_int = 1\n",
    "    \n",
    "    pst_lst = []\n",
    "    # for each of the calls\n",
    "    for i in range(calls):\n",
    "        # for each subreddit\n",
    "        print(\"working on call: \", i)\n",
    "        for j, sub in enumerate(sub_groups):\n",
    "            # If already called pass the after parameter to get latest posts\n",
    "            if i != 0:\n",
    "                params = {'after': aft_lst[j]}\n",
    "            # Call the ReadReddit object to get the posts in a list of dictionaries\n",
    "            posts = ReadRedditPosts()\n",
    "            posts.collect_posts(sub_grp=sub, params = params, headers = headers)\n",
    "            sub_post = posts.posts(features = features)\n",
    "            \n",
    "            #Include comments if flag set and permalink in features\n",
    "            if inc_comm and 'permalink' in features:\n",
    "                for sub_item in sub_post:\n",
    "                    if len(sub_item['permalink']) > 0:\n",
    "                        comm_string = ''\n",
    "                        comm = RedditComments()\n",
    "                        comm.collect_comments(url = sub_item['permalink'], headers = headers)\n",
    "                        # Add comments as one long string separated by three ;\n",
    "                        for comment in comm.comments(features=['body']):\n",
    "                            comm_string += comment['body'] + ';;;'\n",
    "                        sub_item['num_comments_cap'] = comm.no_comments_\n",
    "                        sub_item['comments'] = comm_string\n",
    "                        # pause before hitting the API again\n",
    "                        time.sleep(slp_int)   \n",
    "\n",
    "            pst_lst.extend(sub_post)\n",
    "            # Set the after value for the next call to the API\n",
    "            aft_lst[j] = posts.after_\n",
    "            # pause before hitting the API again\n",
    "            time.sleep(slp_int)   \n",
    "    \n",
    "    # Convert the list to a DataFrame and drop dups\n",
    "    df = pd.DataFrame(pst_lst)\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Write the data to a csv file\n",
    "def write_data(df, data_path):\n",
    "    # assign a unique file name based on the current time\n",
    "    t_stmp = strftime(\"%d%b%Y_%H_%M\", localtime())\n",
    "    o_file = \"posts_\" + t_stmp + \".csv\"\n",
    "    df.to_csv(os.path.join(data_path, o_file), index = False)\n",
    "\n",
    "    \n",
    "# Write the posts data to a SQL table\n",
    "def write_to_database(df, engine = None, table = None):\n",
    "    # write posts to the posts table\n",
    "    if engine == None:\n",
    "        return\n",
    "    else:\n",
    "        df.to_sql(table, con=engine, index=False, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadRedditPosts:\n",
    "    # Attributes of the data retrieval\n",
    "    url_base = 'http://www.reddit.com/'\n",
    "    url_ = None\n",
    "    no_posts_ = None\n",
    "    status_code_ = None\n",
    "    json_ = None\n",
    "    after_ = None\n",
    "    \n",
    "    # Initialization method\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # method to collect data from posts\n",
    "    def collect_posts(self, sub_grp = None, params = {}, headers = {}):\n",
    "        # Set the URL and save it to the class variable\n",
    "        url = self.url_base + 'r/' + sub_grp + '.json'\n",
    "        self.url_ = url\n",
    "        # Hit the API to get posts from this URL\n",
    "        res = requests.get(url, params = params, headers = headers)\n",
    "        # If 200 return\n",
    "        res_code_ = res.status_code\n",
    "        if res.status_code == 200:\n",
    "            self.json_ = res.json()\n",
    "            self.no_posts_ = len(self.json_['data']['children'])\n",
    "            self.after_ = self.json_['data']['after']\n",
    "            return res.json()\n",
    "        else:\n",
    "            return 'Data retrieval error: status code:' + str(res.status_code)\n",
    "\n",
    "    # Method to return the individual posts as a list    \n",
    "    def return_posts(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data['data']['children']\n",
    "    \n",
    "    # Method to return the dictionary keys for posts\n",
    "    def return_post_keys(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data['data']['children'][0]['data'].keys()\n",
    "\n",
    "    # Method to return a list of dictionaries of posts with specified fields\n",
    "    def posts(self, features = []):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        posts = []\n",
    "        # For every entry in the children posts add a dictionary to the list\n",
    "        for entry in data['data']['children']:\n",
    "            post = {}\n",
    "            # For each item in features create a dictionary key: value pair\n",
    "            for item in features:\n",
    "                try:\n",
    "                    post[item] = entry['data'][item]\n",
    "                except:\n",
    "                    post[item] = ''                   \n",
    "            posts.append(post)\n",
    "        return posts\n",
    "\n",
    "    \n",
    "class RedditComments:\n",
    "    # Attributes of the data retrieval\n",
    "    url_ = None\n",
    "    no_comments_ = None\n",
    "    status_code_ = None\n",
    "    json_ = None\n",
    "  \n",
    "    # Initialization method\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # method to collect data from posts\n",
    "    def collect_comments(self, url = None, params = {}, headers = {}):\n",
    "        # Use the provided URL and save it to the class variable\n",
    "        if url == None:\n",
    "            return\n",
    "        if url[:21] == 'http://www.reddit.com/':\n",
    "            url = url + '.json'\n",
    "        else:\n",
    "            url = 'http://www.reddit.com' + url +'.json'         \n",
    "        self.url_ = url\n",
    "        # Hit the API to get posts from this URL\n",
    "        res = requests.get(url, params = params, headers = headers)\n",
    "        # If 200 return\n",
    "        res_code_ = res.status_code\n",
    "        if res.status_code == 200:\n",
    "            self.json_ = res.json()\n",
    "            self.no_comments_ = len(self.json_[1]['data']['children'])\n",
    "            return res.json()\n",
    "        else:\n",
    "            return 'Data retrieval error: status code:' + str(res.status_code)\n",
    "\n",
    "    # Method to return the individual posts as a list    \n",
    "    def return_comments(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data[1]['data']['children']\n",
    "    \n",
    "    # Method to return the dictionary keys for posts\n",
    "    def return_comment_keys(self):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        # Return the children posts\n",
    "        return data[1]['data']['children'][0]['data'].keys()\n",
    "\n",
    "    # Method to return a list of dictionaries of posts with specified fields\n",
    "    def comments(self, features = []):\n",
    "        # Refer to the json variable set during collect_posts()\n",
    "        data = self.json_\n",
    "        comments = []\n",
    "        # For every entry in the children posts add a dictionary to the list\n",
    "        for entry in data[1]['data']['children']:\n",
    "            comment = {}\n",
    "            # For each item in features create a dictionary key: value pair\n",
    "            for item in features:\n",
    "                try:\n",
    "                    comment[item] = entry['data'][item]\n",
    "                except:\n",
    "                    comment[item] = ''                   \n",
    "            comments.append(comment)\n",
    "        return comments\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Establish parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters for retrieving reddit posts data\n",
    "sub_groups = ['relationships', 'diy','politics', 'woodworking']\n",
    "inc_list = ['name','subreddit','selftext','created_utc','author_fullname',\n",
    "           'title', 'num_comments','id', 'permalink']\n",
    "# Set relative data path\n",
    "data_path = \"../data\"\n",
    "\n",
    "# Database engine\n",
    "engine = create_engine('postgres://postgres:pass@34.222.13.94:5432')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from the reddit API and write to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on call:  0\n",
      "working on call:  1\n",
      "working on call:  2\n",
      "working on call:  3\n",
      "working on call:  4\n"
     ]
    }
   ],
   "source": [
    "# Return a dataframe of reddit posts and calculate time to run the function\n",
    "start_time = time.time()\n",
    "df =  hit_reddit(sub_groups = sub_groups, features = inc_list, calls = 5, inc_comm = True)\n",
    "write_to_database(df, engine = engine, table = 'posts')\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time:  0:13:23.345185\n"
     ]
    }
   ],
   "source": [
    "# Show the running time\n",
    "run_time = str(datetime.timedelta(seconds = end_time - start_time))\n",
    "print(\"run time: \", run_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_comments_cap</th>\n",
       "      <th>permalink</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_tfyvomj</td>\n",
       "      <td>I work in esports writing, including Overwatch...</td>\n",
       "      <td>1.554344e+09</td>\n",
       "      <td>b9701d</td>\n",
       "      <td>t3_b9701d</td>\n",
       "      <td>134</td>\n",
       "      <td>58</td>\n",
       "      <td>/r/relationships/comments/b9701d/boyfriends24_...</td>\n",
       "      <td>I have been in a relationship with my boyfrien...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>Boyfriend's(24 M) unemployed pursuit of esport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_3jcizsoz</td>\n",
       "      <td>Maybe he's like \"Well why doesn't she want to ...</td>\n",
       "      <td>1.554369e+09</td>\n",
       "      <td>b9acw7</td>\n",
       "      <td>t3_b9acw7</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>/r/relationships/comments/b9acw7/my_22f_boyfri...</td>\n",
       "      <td>Jfc this is an embarrassing post. \\n\\nMy bf Jo...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>My (22F) boyfriend (22M) has been acting awkwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_3jbr9pcs</td>\n",
       "      <td>It sounds like hormones. Did she take anything...</td>\n",
       "      <td>1.554341e+09</td>\n",
       "      <td>b96ivu</td>\n",
       "      <td>t3_b96ivu</td>\n",
       "      <td>71</td>\n",
       "      <td>34</td>\n",
       "      <td>/r/relationships/comments/b96ivu/m23_my_relati...</td>\n",
       "      <td>Me and my girlfriend are coming close to datin...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>(M23) My relationship with my GF (F22) has bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_3armxrc3</td>\n",
       "      <td>Stop wasting the best years on your life on th...</td>\n",
       "      <td>1.554345e+09</td>\n",
       "      <td>b9740h</td>\n",
       "      <td>t3_b9740h</td>\n",
       "      <td>61</td>\n",
       "      <td>32</td>\n",
       "      <td>/r/relationships/comments/b9740h/am_i_24f_an_a...</td>\n",
       "      <td>So basically the gist is my bf and I have been...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>Am I [24F] an asshole for being grossed out by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_3jcrrl91</td>\n",
       "      <td>&amp;gt;&amp;gt;openly sexualizes them to friends\\n\\nG...</td>\n",
       "      <td>1.554347e+09</td>\n",
       "      <td>b97i2s</td>\n",
       "      <td>t3_b97i2s</td>\n",
       "      <td>46</td>\n",
       "      <td>30</td>\n",
       "      <td>/r/relationships/comments/b97i2s/my_so_26_m_is...</td>\n",
       "      <td>He and I have been together on and off almost ...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>My SO (26, M) is a licensed massage therapist ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_fullname                                           comments  \\\n",
       "0      t2_tfyvomj  I work in esports writing, including Overwatch...   \n",
       "1     t2_3jcizsoz  Maybe he's like \"Well why doesn't she want to ...   \n",
       "2     t2_3jbr9pcs  It sounds like hormones. Did she take anything...   \n",
       "3     t2_3armxrc3  Stop wasting the best years on your life on th...   \n",
       "4     t2_3jcrrl91  &gt;&gt;openly sexualizes them to friends\\n\\nG...   \n",
       "\n",
       "    created_utc      id       name  num_comments  num_comments_cap  \\\n",
       "0  1.554344e+09  b9701d  t3_b9701d           134                58   \n",
       "1  1.554369e+09  b9acw7  t3_b9acw7            25                13   \n",
       "2  1.554341e+09  b96ivu  t3_b96ivu            71                34   \n",
       "3  1.554345e+09  b9740h  t3_b9740h            61                32   \n",
       "4  1.554347e+09  b97i2s  t3_b97i2s            46                30   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/relationships/comments/b9701d/boyfriends24_...   \n",
       "1  /r/relationships/comments/b9acw7/my_22f_boyfri...   \n",
       "2  /r/relationships/comments/b96ivu/m23_my_relati...   \n",
       "3  /r/relationships/comments/b9740h/am_i_24f_an_a...   \n",
       "4  /r/relationships/comments/b97i2s/my_so_26_m_is...   \n",
       "\n",
       "                                            selftext      subreddit  \\\n",
       "0  I have been in a relationship with my boyfrien...  relationships   \n",
       "1  Jfc this is an embarrassing post. \\n\\nMy bf Jo...  relationships   \n",
       "2  Me and my girlfriend are coming close to datin...  relationships   \n",
       "3  So basically the gist is my bf and I have been...  relationships   \n",
       "4  He and I have been together on and off almost ...  relationships   \n",
       "\n",
       "                                               title  \n",
       "0  Boyfriend's(24 M) unemployed pursuit of esport...  \n",
       "1  My (22F) boyfriend (22M) has been acting awkwa...  \n",
       "2  (M23) My relationship with my GF (F22) has bec...  \n",
       "3  Am I [24F] an asshole for being grossed out by...  \n",
       "4  My SO (26, M) is a licensed massage therapist ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the resulting DataFrame\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics         127\n",
       "DIY              126\n",
       "relationships    125\n",
       "woodworking       80\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine Value counts of subreddit\n",
    "df['subreddit'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 0 \n",
      "\n",
      "Column: author_fullname  min:                  max:        t2_zyqql  type:          object  null:               0\n",
      "Column:        comments  min:                  max: “How important   type:          object  null:               0\n",
      "Column:     created_utc  min:    1552423822.0  max:    1554382002.0  type:         float64  null:               0\n",
      "Column:              id  min:          b0cwlg  max:          b9c9ms  type:          object  null:               0\n",
      "Column:            name  min:       t3_b0cwlg  max:       t3_b9c9ms  type:          object  null:               0\n",
      "Column:    num_comments  min:               0  max:            4403  type:           int64  null:               0\n",
      "Column: num_comments_ca  min:               0  max:             109  type:           int64  null:               0\n",
      "Column:       permalink  min: /r/DIY/comments  max: /r/woodworking/  type:          object  null:               0\n",
      "Column:        selftext  min:                  max: with thanks to   type:          object  null:               0\n",
      "Column:       subreddit  min:             DIY  max:     woodworking  type:          object  null:               0\n",
      "Column:           title  min: 'Cheat Working   max: ‘You pay and yo  type:          object  null:               0\n"
     ]
    }
   ],
   "source": [
    "# Count number of duplicated rows\n",
    "print(\"Duplicated rows: %d \\n\" % sum([int(i) for i in df.duplicated()]))\n",
    "\n",
    "#Print a summary of DataFrame columns\n",
    "print_summary(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There might be duplicates in 226 rows\n"
     ]
    }
   ],
   "source": [
    "# Look for duplicates in the selftext column\n",
    "print(\"There might be duplicates in %d rows\" % (len(df['selftext']) - len(set(df['selftext']))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
